Metadata-Version: 2.4
Name: cold-harbour-manager
Version: 0.1.1
Summary: Trading automation and analytics for the Cold Harbour stack
Author-email: Cold Harbour <info@coldharbour.local>
Project-URL: Homepage, https://github.com/tradingbot/cold_harbour_web
Requires-Python: >=3.10
Description-Content-Type: text/markdown

# Cold Harbour: Real-Time Trading Analytics Dashboard

**Cold Harbour** is a specialized high-frequency monitoring system designed for algorithmic traders. It serves as a unified "Control Tower" for Alpaca brokerage accounts, providing real-time visibility into positions, order flow, and equity performance that standard broker interfaces often lack.

It bridges the gap between raw execution data and actionable trading insights, acting as the authoritative source of truth for your trading bot's state.

## Preview

![Platform preview](docs/img/preview.png)

## Key Features

### üìä Real-Time Account Analytics
- **Live P&L Tracking:** Streaming mark-to-market valuation of all open positions via ZeroMQ price feeds.
- **Smart Metrics:** Real-time calculation of Sharpe Ratio (10d/21d/63d...), Drawdown, Win Rate, and Risk/Reward ratios.
- **Equity Curve:** High-resolution (1-minute) intraday equity charting alongside historical daily performance.

### üõ°Ô∏è Order Management & Risk
- **Deep Chain Tracing:** Automatically resolves complex order chains (OTO/OCO/Brackets), tracking positions even through multiple order replacements/modifications.
- **Orphan Detection:** Identifies "orphaned" positions that lack Stop-Loss or Take-Profit protection.
- **Data Lake (Ingester):** A standalone service that immutably records every trade event, order state change, and cash flow (dividends, fees) for auditability.

### ‚ö° Technical Architecture
- **Event-Driven UI:** A lightweight Flask dashboard powered by Server-Sent Events (SSE) for sub-second updates without page reloads.
- **Hybrid Data Storage:** Uses **PostgreSQL** for transactional state (orders/positions) and **TimescaleDB** for time-series data (intraday equity, price bars).
- **Secure Connectivity:** Built-in integration with **Cloudflare Access** tunnels to securely expose database connections from Cloud Run or local dev environments without public IPs.

- **Hybrid Pricing Model:** Simultaneously tracks "Strategy Price" (for technical execution) and "Broker Average Cost" (for financial reconciliation), preventing P&L drift while maintaining precise stop-loss logic.
- **Session-Aware Architecture:** An autonomous `Schedule Supervisor` manages the lifecycle of background workers, automatically spinning up resources for Pre-Market and shutting down after Post-Market close.

---

## System Components

The following architecture diagram summarizes the key services and data
flows:

```mermaid
graph TD
    %% Node Styling
    classDef external fill:#f9f,stroke:#333,stroke-width:2px;
    classDef core fill:#ccf,stroke:#333,stroke-width:2px;
    classDef worker fill:#dfd,stroke:#333,stroke-width:1px;
    classDef logic fill:#ffd,stroke:#333,stroke-width:1px;
    classDef db fill:#eee,stroke:#333,stroke-width:2px;

    %% --- EXTERNAL SOURCES ---
    subgraph External_Sources [External Sources]
        AlpacaWS(Alpaca WebSocket<br/>Trade Updates)
        AlpacaREST(Alpaca REST API<br/>Snapshots & History)
        ZMQ(ZMQ Price Stream<br/>Real-time Quotes)
    end

    %% --- ACCOUNT MANAGER SERVICE ---
    subgraph Account_Manager_Service [Account Manager Service]
        
        %% Entry Point
        Runtime(<b>runtime.py</b><br/>Orchestrator & Config):::core

        %% Background Tasks
        subgraph Background_Workers [Workers / workers.py]
            PriceList(price_listener):::worker
            DBWork(db_worker):::worker
            SnapLoop(snapshot_loop):::worker
            ClosedSync(closed_trades_worker):::worker
            Metrics(metrics_worker):::worker
            Equity(equity_workers):::worker
            SchedSup(schedule_supervisor):::worker
        end

        %% Business Logic
        subgraph Core_Logic [Business Logic Modules]
            Trades(<b>trades.py</b><br/>Order Logic):::logic
            State(<b>state.py</b><br/>In-Memory State):::logic
            Snapshot(<b>snapshot.py</b><br/>Reconciliation):::logic
            EquityLogic(equity.py<br/>Math & Calc):::logic
        end
    end

    %% --- STORAGE & UI ---
    subgraph Storage_and_Output [Storage & Notifications]
        TblLive[(tbl_live<br/>Open Positions)]:::db
        TblClosed[(tbl_closed<br/>History)]:::db
        TblEquity[(tbl_equity<br/>Performance)]:::db
        TblMetrics[(tbl_metrics<br/>JSON KPIs)]:::db
        ChanPos((NOTIFY<br/>pos_channel)):::db
    end

    %% --- DATA FLOW ---
    
    %% Startup
    Runtime -->|Spawns| PriceList
    Runtime -->|Spawns| DBWork
    Runtime -->|Spawns| SnapLoop
    Runtime -->|Spawns| ClosedSync
    Runtime -->|Spawns| SchedSup

    %% Price Stream (ZMQ)
    ZMQ -->|Tick Data| PriceList
    PriceList -->|Update Price| State
    
    %% Trade Stream (Alpaca WS)
    AlpacaWS -->|Fills/Cancels| Trades
    Trades -->|Update/Delete| State
    Trades -->|Insert| TblClosed

    %% Reconciliation (Alpaca REST)
    AlpacaREST -->|Fetch All| Snapshot
    SnapLoop -->|Trigger| Snapshot
    Snapshot -->|Rebuild| State

    %% DB Writes & Notifications
    State -->|Upsert Row| TblLive
    State -->|PG NOTIFY| ChanPos
    DBWork -->|Flush Dirty Rows| State
    
    %% Metrics & Equity
    Metrics -->|Read| TblLive
    Metrics -->|Calc & Write| TblMetrics
    Equity -->|Recalc| EquityLogic
    EquityLogic -->|Write| TblEquity

    %% Apply Styles
    class AlpacaWS,AlpacaREST,ZMQ external;
```

1.  **Account Manager (The Brain):**
    An async Python daemon that maintains the "live" state. It reconciles REST API snapshots with WebSocket streams, calculates Greeks/metrics, and pushes updates to the UI via Postgres `NOTIFY`.

2.  **Data Ingester (The Memory):**
    A robust, self-healing service that creates a raw data lake. It uses **Synthetic IDs**
    (`Timestamp::ExecutionID`) to seamlessly deduplicate high-speed WebSocket events
    against REST API history, ensuring 100% data integrity even during connection
    drops.

3.  **Web Dashboard (The View):**
    A concise, single-page application (`account_positions.html`) rendering live tables and charts. It features visual P&L flashing, "Break-Even" status indicators, and multi-account switching.

## Repository Layout

- `src/cold_harbour/` ‚Äì Application source code.
  - `services/account_manager/` ‚Äì Core trading logic and state management.
  - `services/activity_ingester/` ‚Äì Raw data capture service.
  - `web/` ‚Äì Flask blueprint and Jinja2 templates.
  - `core/` ‚Äì Shared financial math (Smart Sharpe, FIFO/LIFO matching).
- `docs/` ‚Äì Detailed architectural documentation.
- `docker-compose.yml` ‚Äì Full local development stack (Manager + Web + Ingester).

## Getting Started

### Prerequisites
- Docker & Docker Compose.
- Access to an Alpaca Trading Account (Live or Paper).
- A Postgres+TimescaleDB instance (local or remote).

### Quick Start (Local)

1.  **Configure Environment:**
    Copy the template `.env` (not included in git) and populate your credentials:
    ```bash
    ALPACA_API_KEY_LIVE=...
    ALPACA_SECRET_KEY_LIVE=...
    POSTGRESQL_LIVE_LOCAL_CONN_STRING=postgresql://user:pass@localhost:5433/db
    ```

2.  **Launch Stack:**
    ```bash
    docker compose up --build
    ```
    This spins up the *Manager* (syncing state), the *Ingester* (archiving history), and the *Web UI*.

3.  **Access Dashboard:**
    Open `http://localhost:5000` to view your account metrics.

## Packaging for reuse

This repository already exposes its core modules as a Python package
named `coldharbor_manager`. Run `pip install .` (or `pip install -e .`
for editable installs) from the project root to make the package
available to other applications. The package simply extends the legacy
`cold_harbour` directory, so clients can import either
`coldharbor_manager.services.account_manager` or the legacy path
`cold_harbour.services.account_manager`, whichever suits their code
base.

## Documentation

For deep dives into specific subsystems, refer to the `docs/` directory:

- [**Core Analytics**](docs/core_analytics.md): How trades are matched (FIFO vs. Lot) and how "Deep Chain" tracing works.
- [**Account Manager**](docs/account_manager.md): Architecture of the state machine and background workers.
- [**Data Ingester**](docs/ingester.md): Explanation of the immutable data lake and healing strategies.
- [**Web Architecture**](docs/web_architecture.md): How the SSE streaming and caching layers function.

## Deployment

The system is designed for **Google Cloud Run**. The `cloudbuild.yaml` pipeline builds the container and deploys it with secrets injected from Secret Manager. `entrypoint.sh` automatically handles Cloudflare Tunnel initialization before starting the application.

## Contributing

Please refer to [`CONTRIBUTING.md`](CONTRIBUTING.md) for branch naming conventions and Pull Request workflows.
